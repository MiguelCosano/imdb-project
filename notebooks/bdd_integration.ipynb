{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8213e8e1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 1/2\n",
      " \u001b[32m✔\u001b[0m Network imdb-project_default  \u001b[32mCreated\u001b[0m                                   \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Container imdb-db             Starting                                  \u001b[34m0.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 2/2\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network imdb-project_default  \u001b[32mCreated\u001b[0m                                   \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container imdb-db             \u001b[32mStarted\u001b[0m                                   \u001b[34m0.1s \u001b[0m\n",
      "\u001b[?25hNAME      IMAGE         COMMAND                  SERVICE   CREATED                  STATUS                                     PORTS\n",
      "imdb-db   postgres:18   \"docker-entrypoint.s…\"   db        Less than a second ago   Up Less than a second (health: starting)   0.0.0.0:5432->5432/tcp, [::]:5432->5432/tcp\n"
     ]
    }
   ],
   "source": [
    "!docker-compose up -d db\n",
    "\n",
    "!docker-compose ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "573e96c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 22:18:37,152 - INFO - \n",
      "      nconst      primaryName birthYear                  primaryProfession\n",
      "0  nm0000001     Fred Astaire      1899       actor,miscellaneous,producer\n",
      "1  nm0000002    Lauren Bacall      1924   actress,miscellaneous,soundtrack\n",
      "2  nm0000003  Brigitte Bardot      1934  actress,music_department,producer\n",
      "3  nm0000004     John Belushi      1949      actor,writer,music_department\n",
      "4  nm0000005   Ingmar Bergman      1918              writer,director,actor\n",
      "5  nm0000006   Ingrid Bergman      1915        actress,producer,soundtrack\n",
      "6  nm0000007  Humphrey Bogart      1899       actor,producer,miscellaneous\n",
      "7  nm0000008    Marlon Brando      1924              actor,director,writer\n",
      "8  nm0000009   Richard Burton      1925            actor,producer,director\n",
      "9  nm0000010     James Cagney      1899            actor,director,producer\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import requests\n",
    "import shutil\n",
    "import os\n",
    "from sys import path\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "path.append(os.path.realpath(\"../\"))\n",
    "load_dotenv()\n",
    "\n",
    "NAME_BASIC_URL = \"https://datasets.imdbws.com/name.basics.tsv.gz\"\n",
    "\n",
    "NAMES_ZIP_FILE = \"name.basics.tsv.gz\"\n",
    "NAMES_FILE = \"name.basics.tsv\"\n",
    "\n",
    "if not os.path.isfile(NAMES_FILE):\n",
    "    response = requests.get(NAME_BASIC_URL, stream=True)\n",
    "    with open(NAMES_ZIP_FILE, \"wb\") as f:\n",
    "        shutil.copyfileobj(response.raw, f)\n",
    "\n",
    "    with gzip.open(\"name.basics.tsv.gz\", \"rb\") as f_in:\n",
    "        with open(\"name.basics.tsv\", \"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "df_actors = pd.read_csv(\"name.basics.tsv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "selected_columns = [\n",
    "    \"nconst\",\n",
    "    \"primaryName\",\n",
    "    \"birthYear\",\n",
    "    \"primaryProfession\"\n",
    "]\n",
    "clean_df = df_actors[selected_columns]\n",
    "engine = create_engine(os.getenv(\"DATABASE_URL\"))\n",
    "try:\n",
    "    table_name = \"actors\"\n",
    "    clean_df.to_sql(table_name,engine, if_exists=\"replace\", index=False)\n",
    "    table = pd.read_sql(\"SELECT * FROM actors LIMIT 10\", engine)\n",
    "    logging.info(f\"\\n{table}\") \n",
    "except Exception as e:\n",
    "    logging.error(\"Exception:\", e)\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa14e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 23:31:21,927 - INFO - Reading TSV file in chunks...\n",
      "2025-12-06 23:31:22,195 - INFO - Inserted chunk 1 (100000 rows)\n",
      "2025-12-06 23:31:22,359 - INFO - Inserted chunk 2 (100000 rows)\n",
      "2025-12-06 23:31:22,527 - INFO - Inserted chunk 3 (100000 rows)\n",
      "2025-12-06 23:31:22,693 - INFO - Inserted chunk 4 (100000 rows)\n",
      "2025-12-06 23:31:22,858 - INFO - Inserted chunk 5 (100000 rows)\n",
      "2025-12-06 23:31:23,027 - INFO - Inserted chunk 6 (100000 rows)\n",
      "2025-12-06 23:31:23,194 - INFO - Inserted chunk 7 (100000 rows)\n",
      "2025-12-06 23:31:23,365 - INFO - Inserted chunk 8 (100000 rows)\n",
      "2025-12-06 23:31:23,534 - INFO - Inserted chunk 9 (100000 rows)\n",
      "2025-12-06 23:31:23,686 - INFO - Inserted chunk 10 (100000 rows)\n",
      "2025-12-06 23:31:23,838 - INFO - Inserted chunk 11 (100000 rows)\n",
      "2025-12-06 23:31:23,987 - INFO - Inserted chunk 12 (100000 rows)\n",
      "2025-12-06 23:31:24,138 - INFO - Inserted chunk 13 (100000 rows)\n",
      "2025-12-06 23:31:24,286 - INFO - Inserted chunk 14 (100000 rows)\n",
      "2025-12-06 23:31:24,437 - INFO - Inserted chunk 15 (100000 rows)\n",
      "2025-12-06 23:31:24,583 - INFO - Inserted chunk 16 (100000 rows)\n",
      "2025-12-06 23:31:24,730 - INFO - Inserted chunk 17 (100000 rows)\n",
      "2025-12-06 23:31:24,879 - INFO - Inserted chunk 18 (100000 rows)\n",
      "2025-12-06 23:31:25,026 - INFO - Inserted chunk 19 (100000 rows)\n",
      "2025-12-06 23:31:25,176 - INFO - Inserted chunk 20 (100000 rows)\n",
      "2025-12-06 23:31:25,322 - INFO - Inserted chunk 21 (100000 rows)\n",
      "2025-12-06 23:31:25,470 - INFO - Inserted chunk 22 (100000 rows)\n",
      "2025-12-06 23:31:25,609 - INFO - Inserted chunk 23 (100000 rows)\n",
      "2025-12-06 23:31:25,750 - INFO - Inserted chunk 24 (100000 rows)\n",
      "2025-12-06 23:31:25,891 - INFO - Inserted chunk 25 (100000 rows)\n",
      "2025-12-06 23:31:26,038 - INFO - Inserted chunk 26 (100000 rows)\n",
      "2025-12-06 23:31:26,182 - INFO - Inserted chunk 27 (100000 rows)\n",
      "2025-12-06 23:31:26,326 - INFO - Inserted chunk 28 (100000 rows)\n",
      "2025-12-06 23:31:26,477 - INFO - Inserted chunk 29 (100000 rows)\n",
      "2025-12-06 23:31:26,623 - INFO - Inserted chunk 30 (100000 rows)\n",
      "2025-12-06 23:31:26,771 - INFO - Inserted chunk 31 (100000 rows)\n",
      "2025-12-06 23:31:26,919 - INFO - Inserted chunk 32 (100000 rows)\n",
      "2025-12-06 23:31:27,066 - INFO - Inserted chunk 33 (100000 rows)\n",
      "2025-12-06 23:31:27,213 - INFO - Inserted chunk 34 (100000 rows)\n",
      "2025-12-06 23:31:27,358 - INFO - Inserted chunk 35 (100000 rows)\n",
      "2025-12-06 23:31:27,501 - INFO - Inserted chunk 36 (100000 rows)\n",
      "2025-12-06 23:31:27,645 - INFO - Inserted chunk 37 (100000 rows)\n",
      "2025-12-06 23:31:27,791 - INFO - Inserted chunk 38 (100000 rows)\n",
      "2025-12-06 23:31:27,937 - INFO - Inserted chunk 39 (100000 rows)\n",
      "2025-12-06 23:31:28,084 - INFO - Inserted chunk 40 (100000 rows)\n",
      "2025-12-06 23:31:28,229 - INFO - Inserted chunk 41 (100000 rows)\n",
      "2025-12-06 23:31:28,378 - INFO - Inserted chunk 42 (100000 rows)\n",
      "2025-12-06 23:31:28,518 - INFO - Inserted chunk 43 (100000 rows)\n",
      "2025-12-06 23:31:28,659 - INFO - Inserted chunk 44 (100000 rows)\n",
      "2025-12-06 23:31:28,803 - INFO - Inserted chunk 45 (100000 rows)\n",
      "2025-12-06 23:31:28,945 - INFO - Inserted chunk 46 (100000 rows)\n",
      "2025-12-06 23:31:29,090 - INFO - Inserted chunk 47 (100000 rows)\n",
      "2025-12-06 23:31:29,234 - INFO - Inserted chunk 48 (100000 rows)\n",
      "2025-12-06 23:31:29,378 - INFO - Inserted chunk 49 (100000 rows)\n",
      "2025-12-06 23:31:29,524 - INFO - Inserted chunk 50 (100000 rows)\n",
      "2025-12-06 23:31:29,668 - INFO - Inserted chunk 51 (100000 rows)\n",
      "2025-12-06 23:31:29,814 - INFO - Inserted chunk 52 (100000 rows)\n",
      "2025-12-06 23:31:29,958 - INFO - Inserted chunk 53 (100000 rows)\n",
      "2025-12-06 23:31:30,104 - INFO - Inserted chunk 54 (100000 rows)\n",
      "2025-12-06 23:31:30,249 - INFO - Inserted chunk 55 (100000 rows)\n",
      "2025-12-06 23:31:30,391 - INFO - Inserted chunk 56 (100000 rows)\n",
      "2025-12-06 23:31:30,536 - INFO - Inserted chunk 57 (100000 rows)\n",
      "2025-12-06 23:31:30,681 - INFO - Inserted chunk 58 (100000 rows)\n",
      "2025-12-06 23:31:30,824 - INFO - Inserted chunk 59 (100000 rows)\n",
      "2025-12-06 23:31:30,965 - INFO - Inserted chunk 60 (100000 rows)\n",
      "2025-12-06 23:31:31,137 - INFO - Inserted chunk 61 (100000 rows)\n",
      "2025-12-06 23:31:31,288 - INFO - Inserted chunk 62 (100000 rows)\n",
      "2025-12-06 23:31:31,441 - INFO - Inserted chunk 63 (100000 rows)\n",
      "2025-12-06 23:31:31,594 - INFO - Inserted chunk 64 (100000 rows)\n",
      "2025-12-06 23:31:31,748 - INFO - Inserted chunk 65 (100000 rows)\n",
      "2025-12-06 23:31:31,907 - INFO - Inserted chunk 66 (100000 rows)\n",
      "2025-12-06 23:31:32,060 - INFO - Inserted chunk 67 (100000 rows)\n",
      "2025-12-06 23:31:32,214 - INFO - Inserted chunk 68 (100000 rows)\n",
      "2025-12-06 23:31:32,399 - INFO - Inserted chunk 69 (100000 rows)\n",
      "2025-12-06 23:31:32,554 - INFO - Inserted chunk 70 (100000 rows)\n",
      "2025-12-06 23:31:32,695 - INFO - Inserted chunk 71 (100000 rows)\n",
      "2025-12-06 23:31:32,834 - INFO - Inserted chunk 72 (100000 rows)\n",
      "2025-12-06 23:31:32,974 - INFO - Inserted chunk 73 (100000 rows)\n",
      "2025-12-06 23:31:33,117 - INFO - Inserted chunk 74 (100000 rows)\n",
      "2025-12-06 23:31:33,258 - INFO - Inserted chunk 75 (100000 rows)\n",
      "2025-12-06 23:31:33,399 - INFO - Inserted chunk 76 (100000 rows)\n",
      "2025-12-06 23:31:33,543 - INFO - Inserted chunk 77 (100000 rows)\n",
      "2025-12-06 23:31:33,685 - INFO - Inserted chunk 78 (100000 rows)\n",
      "2025-12-06 23:31:33,830 - INFO - Inserted chunk 79 (100000 rows)\n",
      "2025-12-06 23:31:33,971 - INFO - Inserted chunk 80 (100000 rows)\n",
      "2025-12-06 23:31:34,114 - INFO - Inserted chunk 81 (100000 rows)\n",
      "2025-12-06 23:31:34,276 - INFO - Inserted chunk 82 (100000 rows)\n",
      "2025-12-06 23:31:34,438 - INFO - Inserted chunk 83 (100000 rows)\n",
      "2025-12-06 23:31:34,597 - INFO - Inserted chunk 84 (100000 rows)\n",
      "2025-12-06 23:31:34,754 - INFO - Inserted chunk 85 (100000 rows)\n",
      "2025-12-06 23:31:34,908 - INFO - Inserted chunk 86 (100000 rows)\n",
      "2025-12-06 23:31:35,063 - INFO - Inserted chunk 87 (100000 rows)\n",
      "2025-12-06 23:31:35,219 - INFO - Inserted chunk 88 (100000 rows)\n",
      "2025-12-06 23:31:35,370 - INFO - Inserted chunk 89 (100000 rows)\n",
      "2025-12-06 23:31:35,525 - INFO - Inserted chunk 90 (100000 rows)\n",
      "2025-12-06 23:31:35,683 - INFO - Inserted chunk 91 (100000 rows)\n",
      "2025-12-06 23:31:35,839 - INFO - Inserted chunk 92 (100000 rows)\n",
      "2025-12-06 23:31:36,276 - INFO - Inserted chunk 93 (100000 rows)\n",
      "2025-12-06 23:31:36,437 - INFO - Inserted chunk 94 (100000 rows)\n",
      "2025-12-06 23:31:36,591 - INFO - Inserted chunk 95 (100000 rows)\n",
      "2025-12-06 23:31:36,745 - INFO - Inserted chunk 96 (100000 rows)\n",
      "2025-12-06 23:31:36,902 - INFO - Inserted chunk 97 (100000 rows)\n",
      "2025-12-06 23:31:37,060 - INFO - Inserted chunk 98 (100000 rows)\n",
      "2025-12-06 23:31:37,215 - INFO - Inserted chunk 99 (100000 rows)\n",
      "2025-12-06 23:31:37,367 - INFO - Inserted chunk 100 (100000 rows)\n",
      "2025-12-06 23:31:37,520 - INFO - Inserted chunk 101 (100000 rows)\n",
      "2025-12-06 23:31:37,673 - INFO - Inserted chunk 102 (100000 rows)\n",
      "2025-12-06 23:31:37,830 - INFO - Inserted chunk 103 (100000 rows)\n",
      "2025-12-06 23:31:37,987 - INFO - Inserted chunk 104 (100000 rows)\n",
      "2025-12-06 23:31:38,139 - INFO - Inserted chunk 105 (100000 rows)\n",
      "2025-12-06 23:31:38,292 - INFO - Inserted chunk 106 (100000 rows)\n",
      "2025-12-06 23:31:38,449 - INFO - Inserted chunk 107 (100000 rows)\n",
      "2025-12-06 23:31:38,602 - INFO - Inserted chunk 108 (100000 rows)\n",
      "2025-12-06 23:31:38,755 - INFO - Inserted chunk 109 (100000 rows)\n",
      "2025-12-06 23:31:38,908 - INFO - Inserted chunk 110 (100000 rows)\n",
      "2025-12-06 23:31:39,059 - INFO - Inserted chunk 111 (100000 rows)\n",
      "2025-12-06 23:31:39,209 - INFO - Inserted chunk 112 (100000 rows)\n",
      "2025-12-06 23:31:39,356 - INFO - Inserted chunk 113 (100000 rows)\n",
      "2025-12-06 23:31:39,504 - INFO - Inserted chunk 114 (100000 rows)\n",
      "2025-12-06 23:31:39,653 - INFO - Inserted chunk 115 (100000 rows)\n",
      "2025-12-06 23:31:39,804 - INFO - Inserted chunk 116 (100000 rows)\n",
      "2025-12-06 23:31:39,951 - INFO - Inserted chunk 117 (100000 rows)\n",
      "2025-12-06 23:31:40,105 - INFO - Inserted chunk 118 (100000 rows)\n",
      "2025-12-06 23:31:40,255 - INFO - Inserted chunk 119 (100000 rows)\n",
      "2025-12-06 23:31:40,407 - INFO - Inserted chunk 120 (100000 rows)\n",
      "2025-12-06 23:31:40,558 - INFO - Inserted chunk 121 (100000 rows)\n",
      "2025-12-06 23:31:40,707 - INFO - Inserted chunk 122 (100000 rows)\n",
      "2025-12-06 23:31:40,856 - INFO - Inserted chunk 123 (100000 rows)\n",
      "2025-12-06 23:31:41,030 - INFO - Inserted chunk 124 (100000 rows)\n",
      "2025-12-06 23:31:41,175 - INFO - Inserted chunk 125 (100000 rows)\n",
      "2025-12-06 23:31:41,320 - INFO - Inserted chunk 126 (100000 rows)\n",
      "2025-12-06 23:31:41,467 - INFO - Inserted chunk 127 (100000 rows)\n",
      "2025-12-06 23:31:41,617 - INFO - Inserted chunk 128 (100000 rows)\n",
      "2025-12-06 23:31:41,764 - INFO - Inserted chunk 129 (100000 rows)\n",
      "2025-12-06 23:31:41,912 - INFO - Inserted chunk 130 (100000 rows)\n",
      "2025-12-06 23:31:42,058 - INFO - Inserted chunk 131 (100000 rows)\n",
      "2025-12-06 23:31:42,203 - INFO - Inserted chunk 132 (100000 rows)\n",
      "2025-12-06 23:31:42,352 - INFO - Inserted chunk 133 (100000 rows)\n",
      "2025-12-06 23:31:42,497 - INFO - Inserted chunk 134 (100000 rows)\n",
      "2025-12-06 23:31:42,643 - INFO - Inserted chunk 135 (100000 rows)\n",
      "2025-12-06 23:31:42,790 - INFO - Inserted chunk 136 (100000 rows)\n",
      "2025-12-06 23:31:42,941 - INFO - Inserted chunk 137 (100000 rows)\n",
      "2025-12-06 23:31:43,088 - INFO - Inserted chunk 138 (100000 rows)\n",
      "2025-12-06 23:31:43,233 - INFO - Inserted chunk 139 (100000 rows)\n",
      "2025-12-06 23:31:43,379 - INFO - Inserted chunk 140 (100000 rows)\n",
      "2025-12-06 23:31:43,525 - INFO - Inserted chunk 141 (100000 rows)\n",
      "2025-12-06 23:31:43,668 - INFO - Inserted chunk 142 (100000 rows)\n",
      "2025-12-06 23:31:43,815 - INFO - Inserted chunk 143 (100000 rows)\n",
      "2025-12-06 23:31:43,962 - INFO - Inserted chunk 144 (100000 rows)\n",
      "2025-12-06 23:31:44,099 - INFO - Inserted chunk 145 (100000 rows)\n",
      "2025-12-06 23:31:44,240 - INFO - Inserted chunk 146 (100000 rows)\n",
      "2025-12-06 23:31:44,388 - INFO - Inserted chunk 147 (100000 rows)\n",
      "2025-12-06 23:31:44,539 - INFO - Inserted chunk 148 (100000 rows)\n",
      "2025-12-06 23:31:44,685 - INFO - Inserted chunk 149 (100000 rows)\n",
      "2025-12-06 23:31:44,741 - INFO - Inserted chunk 150 (22973 rows)\n",
      "2025-12-06 23:31:45,885 - INFO - Total rows inserted: 14922973\n",
      "2025-12-06 23:31:45,886 - INFO - Database connection closed\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import shutil\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "from io import StringIO\n",
    "import csv\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "sys.path.append(os.path.realpath(\"../\"))\n",
    "load_dotenv()\n",
    "\n",
    "NAME_BASIC_URL = \"https://datasets.imdbws.com/name.basics.tsv.gz\"\n",
    "NAMES_ZIP_FILE = \"name.basics.tsv.gz\"\n",
    "NAMES_FILE = \"name.basics.tsv\"\n",
    "\n",
    "if not os.path.isfile(NAMES_FILE):\n",
    "    logging.info(\"Downloading name.basics.tsv.gz...\")\n",
    "    response = requests.get(NAME_BASIC_URL, stream=True)\n",
    "    with open(NAMES_ZIP_FILE, \"wb\") as f:\n",
    "        shutil.copyfileobj(response.raw, f)\n",
    "    \n",
    "    logging.info(\"Extracting gzip file...\")\n",
    "    with gzip.open(NAMES_ZIP_FILE, \"rb\") as f_in:\n",
    "        with open(NAMES_FILE, \"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    logging.info(\"Download complete\")\n",
    "\n",
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    \"\"\"\n",
    "    Execute SQL statement inserting data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table : pandas.io.sql.SQLTable\n",
    "    conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n",
    "    keys : list of str\n",
    "        Column names\n",
    "    data_iter : Iterable that iterates the values to be inserted\n",
    "    \"\"\"\n",
    "    dbapi_conn = conn.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek(0)\n",
    "\n",
    "        columns = ', '.join(['\"{}\"'.format(k) for k in keys])\n",
    "        if table.schema:\n",
    "            table_name = '{}.{}'.format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n",
    "            table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)\n",
    "\n",
    "engine = create_engine(os.getenv(\"DATABASE_URL\"))\n",
    "table_name = \"actors\"\n",
    "\n",
    "try:\n",
    "    logging.info(\"Reading TSV file in chunks...\")\n",
    "    selected_columns = [\"nconst\", \"primaryName\", \"birthYear\", \"primaryProfession\"]\n",
    "    chunk_size = 100000\n",
    "    \n",
    "    for i, chunk in enumerate(pd.read_csv(\n",
    "        NAMES_FILE, \n",
    "        sep=\"\\t\", \n",
    "        usecols=selected_columns,\n",
    "        chunksize=chunk_size,\n",
    "        na_values=['\\\\N'],\n",
    "        keep_default_na=True,\n",
    "        dtype={\n",
    "        'nconst': str,\n",
    "        'primaryName': str,\n",
    "        'birthYear': 'Int64',\n",
    "        'primaryProfession': str\n",
    "    }\n",
    "    )):\n",
    "        chunk = chunk.where(pd.notnull(chunk), None)\n",
    "\n",
    "        if_exists_mode = 'replace' if i == 0 else 'append'\n",
    "        \n",
    "        chunk.to_sql(\n",
    "            table_name,\n",
    "            engine,\n",
    "            if_exists=if_exists_mode,\n",
    "            index=False,\n",
    "            method=psql_insert_copy\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Inserted chunk {i+1} ({len(chunk)} rows)\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = pd.read_sql(\"SELECT COUNT(*) as total FROM actors\", conn)\n",
    "        logging.info(f\"Total rows inserted: {result['total'][0]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f\"Exception occurred: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    engine.dispose()\n",
    "    logging.info(\"Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6b6a20",
   "metadata": {},
   "source": [
    "## Data insertion\n",
    "\n",
    "After reviewing several options to insert the data into the postgressql database the best approach would be the copy one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5f88aee",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 0/1\n",
      " \u001b[33m⠋\u001b[0m Container imdb-db  Stopping                                             \u001b[34m0.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/1\n",
      " \u001b[33m⠙\u001b[0m Container imdb-db  Stopping                                             \u001b[34m0.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 0/1\n",
      " \u001b[33m⠹\u001b[0m Container imdb-db  Stopping                                             \u001b[34m0.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 1/3\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container imdb-db                  \u001b[32mRemoved\u001b[0m                              \u001b[34m0.3s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Volume imdb-project_postgres_data  Removing                             \u001b[34m0.1s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Network imdb-project_default       Re...                                \u001b[34m0.1s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container imdb-db                  \u001b[32mRemoved\u001b[0m                              \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Volume imdb-project_postgres_data  \u001b[32mRemoved\u001b[0m                              \u001b[34m0.1s \u001b[0m\n",
      " \u001b[33m⠙\u001b[0m Network imdb-project_default       Re...                                \u001b[34m0.2s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 2/3\n",
      " \u001b[32m✔\u001b[0m Container imdb-db                  \u001b[32mRemoved\u001b[0m                              \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Volume imdb-project_postgres_data  \u001b[32mRemoved\u001b[0m                              \u001b[34m0.1s \u001b[0m\n",
      " \u001b[33m⠹\u001b[0m Network imdb-project_default       Re...                                \u001b[34m0.3s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 3/3\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container imdb-db                  \u001b[32mRemoved\u001b[0m                              \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Volume imdb-project_postgres_data  \u001b[32mRemoved\u001b[0m                              \u001b[34m0.1s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Network imdb-project_default       \u001b[32mRe...\u001b[0m                                \u001b[34m0.3s \u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker-compose down -v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
